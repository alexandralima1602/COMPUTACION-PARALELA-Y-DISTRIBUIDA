{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c04aa418-a91a-4752-b84d-e69a652915ee",
   "metadata": {},
   "source": [
    "### Modelos de memoria distribuida vs. memoria compartida\n",
    "\n",
    "En la computación de alto rendimiento, los modelos de memoria desempeñan un papel crucial en la forma en que los sistemas gestionan y acceden a los datos. Dos modelos prominentes son la memoria compartida y la memoria distribuida. Ambos tienen sus ventajas y desafíos únicos y son adecuados para diferentes tipos de aplicaciones y arquitecturas de sistemas.\n",
    "\n",
    "**Memoria compartida**\n",
    "\n",
    "En el modelo de memoria compartida, múltiples procesadores tienen acceso a una región común de memoria. Este enfoque es característico de las arquitecturas multiprocesador simétrico (SMP), donde todos los procesadores comparten la misma memoria física y pueden leer y escribir en ella. Las principales características y desafíos del modelo de memoria compartida son:\n",
    "\n",
    "- Acceso Uniforme: Los procesadores pueden acceder a cualquier ubicación de la memoria compartida con el mismo tiempo de acceso. Esto facilita la programación, ya que los desarrolladores pueden escribir programas sin preocuparse de la ubicación física de los datos.\n",
    "\n",
    "- Coherencia de Caché: Uno de los mayores desafíos en los sistemas de memoria compartida es mantener la coherencia de caché. Dado que múltiples procesadores pueden almacenar en caché los mismos datos, es crucial asegurarse de que cualquier modificación se refleje en todas las cachés. Protocolos como MESI (Modificado, Exclusivo, Compartido, Inválido) y MOESI (Modificado, Exclusivo, Compartido, Inválido, Propietario) se utilizan para gestionar esta coherencia.\n",
    "\n",
    "- Sincronización: La sincronización es vital en los sistemas de memoria compartida para prevenir condiciones de carrera y garantizar la consistencia de los datos. Los mecanismos comunes de sincronización incluyen semáforos, mutexes y barreras. Aunque estos mecanismos son efectivos, pueden introducir sobrecarga y complicar la programación.\n",
    "\n",
    "- Escalabilidad: La escalabilidad es un desafío significativo en los sistemas de memoria compartida. A medida que se añaden más procesadores, el tráfico en el bus de memoria y la complejidad de mantener la coherencia de caché aumentan, lo que puede degradar el rendimiento.\n",
    "\n",
    "- Programación: La programación en sistemas de memoria compartida es generalmente más sencilla debido a la uniformidad en el acceso a la memoria. Los desarrolladores pueden utilizar modelos de programación paralela como OpenMP, que simplifican la creación de aplicaciones paralelas.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "038e1860-4dfd-4222-8990-38b0a2fa4e9e",
   "metadata": {},
   "source": [
    "**Estrategias de memoria compartida**\n",
    "\n",
    "En sistemas multiprocesador donde múltiples procesadores acceden y comparten la misma memoria física, es crucial implementar estrategias efectivas para garantizar el rendimiento, la coherencia y la consistencia de los datos. A continuación, se explican las principales estrategias de memoria compartida, incluyendo la localización de datos, la reducción de contención, y la minimización de la latencia de caché.\n",
    "\n",
    "1 . Localización de datos\n",
    "\n",
    "La localización de datos en sistemas de memoria compartida implica organizar y almacenar los datos de manera que se optimice el acceso a la memoria por parte de los procesadores, minimizando la latencia de acceso y mejorando el rendimiento general del sistema.\n",
    "\n",
    "Ejemplo:\n",
    "En una aplicación de procesamiento de imágenes, almacenar los datos de píxeles que se procesan juntos en ubicaciones contiguas de memoria puede mejorar significativamente el rendimiento debido a un acceso más eficiente a la caché.\n",
    "\n",
    "Técnicas:\n",
    "\n",
    "- Localidad espacial: Los datos que se utilizan juntos deben almacenarse cerca unos de otros en la memoria. Por ejemplo, las estructuras de datos contiguas en matrices.\n",
    "- Localidad temporal: Los datos que se utilizan con frecuencia deben almacenarse en cachés para accesos rápidos. Por ejemplo, variables locales y datos frecuentemente accedidos en cachés L1 o L2.\n",
    "\n",
    "Ventajas:\n",
    "\n",
    "- Acceso rápido: Los procesadores pueden acceder rápidamente a los datos que necesitan, mejorando la eficiencia del sistema.\n",
    "- Mejor utilización de la caché: Almacenar datos relacionados cerca unos de otros maximiza la eficiencia del uso de la caché.\n",
    "\n",
    "Desafíos:\n",
    "\n",
    "- Diseño complejo: Organizar los datos de manera eficiente puede ser complejo y requerir un diseño cuidadoso de la memoria.\n",
    "\n",
    "2 . Reducción de contención\n",
    "\n",
    "La contención ocurre cuando múltiples procesadores intentan acceder simultáneamente a la misma ubicación de memoria, lo que puede causar conflictos y retrasos. Reducir la contención es crucial para mantener un rendimiento óptimo en sistemas de memoria compartida.\n",
    "\n",
    "Ejemplo:\n",
    "\n",
    "En una base de datos concurrente, múltiples transacciones pueden intentar acceder y modificar el mismo registro al mismo tiempo. La contención se puede reducir mediante el uso de particiones y técnicas de control de concurrencia.\n",
    "\n",
    "Técnicas:\n",
    "\n",
    "- Descomposición de tareas: Dividir las tareas en subtareas independientes que acceden a diferentes partes de la memoria, reduciendo la necesidad de acceso concurrente a las mismas ubicaciones.\n",
    "- Bloqueo de granularidad fina: Utilizar bloqueos más específicos (por ejemplo, a nivel de fila en lugar de a nivel de tabla) para reducir la probabilidad de contención.\n",
    "- Algoritmos concurrentes sin bloqueos: Implementar estructuras de datos y algoritmos que minimicen o eliminen el uso de bloqueos, como listas enlazadas sin bloqueo o pilas concurrentes.\n",
    "\n",
    "Ventajas:\n",
    "\n",
    "- Mejor concurrencia: Menos bloqueos y esperas entre procesadores, mejorando la eficiencia y el rendimiento.\n",
    "- Mayor escalabilidad: Permite que el sistema escale mejor con el aumento del número de procesadores.\n",
    "\n",
    "Desafíos:\n",
    "\n",
    "- Complejidad de implementación: Implementar algoritmos concurrentes sin bloqueos puede ser técnicamente desafiante.\n",
    "\n",
    "3 . Minimización de la latencia de caché\n",
    "\n",
    "La latencia de caché se refiere al tiempo que tarda un procesador en acceder a los datos almacenados en caché. Minimizar esta latencia es esencial para maximizar el rendimiento en sistemas de memoria compartida.\n",
    "\n",
    "Ejemplo:\n",
    "\n",
    "En un sistema de procesamiento de transacciones financieras, es crucial acceder rápidamente a los datos de las cuentas. Mantener estos datos en caché reduce la latencia y mejora el rendimiento del sistema.\n",
    "\n",
    "Técnicas:\n",
    "\n",
    "- Prefetching de datos: Anticipar qué datos serán necesarios próximamente y cargarlos en caché antes de que se soliciten.\n",
    "- Políticas de reemplazo de caché: Utilizar políticas eficientes de reemplazo de caché, como LRU (Least Recently Used) para mantener en caché los datos más relevantes.\n",
    "- Agrupación de datos: Agrupar datos que se utilizan conjuntamente para que se almacenen juntos en caché y se accedan de manera más eficiente.\n",
    "- Tamaño óptimo de caché: Determinar el tamaño óptimo de las cachés en diferentes niveles (L1, L2, L3) para equilibrar el espacio de almacenamiento y la latencia de acceso.\n",
    "\n",
    "Ventajas:\n",
    "\n",
    "- Mayor rendimiento: Acceso rápido a datos frecuentemente utilizados, mejorando la eficiencia del sistema.\n",
    "- Reducción de latencia: Menos tiempo de espera para acceder a datos almacenados en caché.\n",
    "\n",
    "Desafíos:\n",
    "\n",
    "- Gestión de caché: Determinar qué datos almacenar y cuándo reemplazarlos puede ser complejo y requerir una gestión cuidadosa.\n",
    "- Coherencia de caché: Mantener la coherencia de los datos en cachés múltiples es un desafío técnico significativo.\n",
    "\n",
    "4 . Coherencia y consistencia de caché\n",
    "\n",
    "La coherencia de caché asegura que todas las copias de un dato en diferentes cachés sean iguales. La consistencia de caché garantiza el orden en que las operaciones de memoria son vistas por diferentes procesadores.\n",
    "\n",
    "Ejemplo:\n",
    "\n",
    "En una aplicación de simulación científica, varios procesadores pueden acceder y actualizar los mismos datos. Es crucial que todos los procesadores vean los mismos datos actualizados para evitar errores en los cálculos.\n",
    "\n",
    "Técnicas:\n",
    "\n",
    "- Protocolos de coherencia (e.g., MESI): Utilizar protocolos de coherencia de caché como MESI (Modificado, Exclusivo, Compartido, Inválido) para asegurar que las actualizaciones de datos se propaguen correctamente a todas las cachés.\n",
    "- Barreras de memoria: Implementar barreras de memoria para asegurar que todas las operaciones de lectura y escritura se completen antes de que se continúen otras operaciones.\n",
    "\n",
    "Ventajas:\n",
    "\n",
    "- Datos consistentes: Asegura que todos los procesadores trabajen con los mismos datos, evitando errores.\n",
    "- Sincronización eficiente: Permite la sincronización eficiente entre múltiples procesadores.\n",
    "\n",
    "Desafíos:\n",
    "\n",
    "- Sobrecarga de comunicación: La propagación de actualizaciones entre cachés puede introducir sobrecarga de comunicación.\n",
    "- Complejidad técnica: Implementar y gestionar protocolos de coherencia puede ser técnicamente desafiante."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a91030-db6a-40b8-95d7-fa5bc3fe1d7e",
   "metadata": {},
   "source": [
    "#### Ejemplos \n",
    "\n",
    "- Un sistema de memoria compartida típico podría ser un servidor de múltiples núcleos donde todos los núcleos pueden acceder a una base de datos en memoria. Si un núcleo actualiza un registro en la base de datos, todos los demás núcleos deben ver esta actualización inmediatamente, lo cual se gestiona a través de mecanismos de coherencia de caché.\n",
    "\n",
    "- Consideremos una aplicación de simulación científica que corre en un servidor SMP. Los diferentes núcleos del servidor pueden trabajar en diferentes partes de la simulación, pero necesitan acceder y actualizar una memoria compartida donde se almacenan las variables globales y los resultados parciales de la simulación. Utilizando OpenMP, los desarrolladores pueden paralelizar el bucle principal de la simulación para que cada núcleo trabaje en diferentes iteraciones "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe206bb-2f9d-4e5b-8930-c7e20393d38e",
   "metadata": {},
   "source": [
    "\n",
    "**Memoria distribuida**\n",
    "\n",
    "En contraste, el modelo de memoria distribuida implica que cada procesador tiene su propia memoria local. Los procesadores se comunican entre sí mediante un mecanismo de paso de mensajes para compartir datos. Este modelo es característico de los sistemas de procesamiento paralelo de memoria distribuida (DMPP) y las arquitecturas de clústeres.\n",
    "\n",
    "- Autonomía de memoria: Cada procesador tiene acceso rápido a su propia memoria local, lo que reduce la latencia de acceso a los datos locales. Sin embargo, acceder a la memoria de otros procesadores requiere comunicación explícita, lo cual puede ser más lento.\n",
    "\n",
    "- Paso de mensajes: La comunicación en sistemas de memoria distribuida se realiza mediante el paso de mensajes. Librerías como MPI (Message Passing Interface) son comunes y proporcionan las herramientas necesarias para la comunicación inter-procesador. Aunque el paso de mensajes introduce sobrecarga, es esencial para la coordinación y el intercambio de datos entre procesadores.\n",
    "\n",
    "- Sincronización: La sincronización en sistemas de memoria distribuida es menos problemática en términos de coherencia de caché, pero sigue siendo crucial para coordinar la ejecución de tareas y el intercambio de datos. Los mensajes deben ser gestionados cuidadosamente para evitar bloqueos y garantizar la entrega correcta.\n",
    "\n",
    "- Escalabilidad: Los sistemas de memoria distribuida generalmente escalan mejor que los de memoria compartida. A medida que se añaden más procesadores, cada uno con su propia memoria local, se reduce la contención por el acceso a la memoria, permitiendo que el sistema maneje un mayor número de procesadores de manera más eficiente.\n",
    "\n",
    "- Programación: La programación en sistemas de memoria distribuida puede ser más compleja debido a la necesidad de gestionar explícitamente la comunicación y la sincronización. MPI es una herramienta comúnmente utilizada para este propósito, pero requiere una comprensión profunda de los patrones de comunicación y la estructura de datos distribuida."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c5fd07-67c8-4c85-b9b6-71dc5e8ee4bf",
   "metadata": {},
   "source": [
    "**Estrategias de memoria distribuida**\n",
    "\n",
    "En sistemas de memoria distribuida, donde cada procesador tiene su propia memoria local y los datos se reparten entre diferentes nodos, es crucial implementar estrategias eficientes para manejar y acceder a los datos. Aquí se explican tres estrategias clave para mejorar el rendimiento en sistemas de memoria distribuida: localización de datos, reducción de contención y minimización de la latencia de caché.\n",
    "\n",
    "1. Localización de datos: La localización de datos se refiere a la estrategia de almacenar los datos lo más cerca posible de los procesadores que los utilizan con mayor frecuencia. Esto minimiza la necesidad de comunicación entre nodos y reduce la latencia de acceso a los datos.\n",
    "\n",
    "Ejemplo:\n",
    "En un sistema de recomendación, los datos de un usuario pueden almacenarse en el nodo donde se realizan los cálculos de recomendación para ese usuario. De esta forma, las consultas y actualizaciones de datos son rápidas y eficientes.\n",
    "\n",
    "Técnicas:\n",
    "\n",
    "- Particionamiento de datos: Dividir los datos en particiones basadas en ciertos criterios (e.g., geográficos, demográficos) y asignar cada partición a un nodo específico.\n",
    "- Replicación de datos: Mantener copias de los datos en múltiples nodos para asegurar que los datos estén disponibles localmente en varios nodos, reduciendo la latencia de acceso.\n",
    "- Consistencia local: Asegurar que las operaciones de lectura/escritura en los datos locales se manejen de manera eficiente, y que las actualizaciones se propaguen a otros nodos según sea necesario.\n",
    "\n",
    "Ventajas:\n",
    "\n",
    "- Menor latencia: Acceso más rápido a los datos debido a su proximidad.\n",
    "- Reducción de tráfico: Menos comunicación entre nodos, reduciendo el tráfico de red.\n",
    "\n",
    "Desafíos:\n",
    "\n",
    "- Equilibrio de carga: Asegurar que los datos y la carga de trabajo estén equilibrados entre los nodos.\n",
    "- Consistencia de datos: Mantener la consistencia de los datos replicados entre diferentes nodos.\n",
    "\n",
    "2. Reducción de contención: La contención ocurre cuando múltiples nodos intentan acceder a los mismos recursos simultáneamente, lo que puede causar conflictos y retrasos. La reducción de contención se centra en minimizar estos conflictos para mejorar el rendimiento del sistema.\n",
    "\n",
    "Ejemplo:\n",
    "En una base de datos distribuida, múltiples nodos pueden intentar acceder y actualizar el mismo registro simultáneamente. La contención se puede reducir mediante técnicas de particionamiento y replicación.\n",
    "\n",
    "Técnicas:\n",
    "\n",
    "- Particionamiento horizontal: Dividir una base de datos en tablas más pequeñas (shards) y asignar cada shard a un nodo diferente, reduciendo la contención en tablas grandes.\n",
    "- Bloqueo de granularidad fina: Utilizar bloqueos más específicos en lugar de bloquear grandes regiones de datos. Por ejemplo, en lugar de bloquear toda una tabla, bloquear solo las filas o columnas específicas que están siendo accedidas.\n",
    "- Control optimista de concurrencia: Permitir que múltiples transacciones se procesen simultáneamente sin bloquearse entre sí y resolver los conflictos solo si ocurren (e.g., utilizando técnicas de versionado).\n",
    "\n",
    "Ventajas:\n",
    "\n",
    "- Mejor utilización del sistema: Menos tiempo de espera para los nodos, aumentando la eficiencia del sistema.\n",
    "- Mayor concurrencia: Permite que más transacciones se procesen simultáneamente.\n",
    "\n",
    "Desafíos:\n",
    "\n",
    "- Complejidad de implementación: Estrategias como el control optimista de concurrencia pueden ser complejas de implementar y gestionar.\n",
    "- Conflictos de datos: Aumenta la posibilidad de conflictos que deben resolverse eficientemente.\n",
    "\n",
    "3. Minimización de la latencia de caché\n",
    "La latencia de caché se refiere al tiempo que tarda un procesador en acceder a los datos almacenados en caché. Minimizar esta latencia es crucial para mejorar el rendimiento del sistema.\n",
    "\n",
    "Ejemplo:\n",
    "\n",
    "En un sistema de procesamiento de datos en tiempo real, como una plataforma de análisis de eventos, es fundamental acceder rápidamente a los datos más recientes. Minimizar la latencia de caché asegura que los análisis se realicen lo más rápido posible.\n",
    "\n",
    "Técnicas:\n",
    "\n",
    "- Prefetching de datos: Anticipar qué datos serán necesarios próximamente y cargarlos en caché antes de que se soliciten, reduciendo el tiempo de espera.\n",
    "- Cache Locality: Organizar los datos en memoria de manera que las operaciones accedan secuencialmente a áreas contiguas de memoria, aprovechando mejor la jerarquía de la memoria caché.\n",
    "- Políticas de reemplazo de caché: Implementar políticas de reemplazo eficientes (e.g., LRU - Least Recently Used) para mantener en caché los datos que se acceden con mayor frecuencia.\n",
    "- Caché distribuida: Implementar una caché distribuida que se extiende a través de múltiples nodos, permitiendo que los datos caché se almacenen en varios lugares para un acceso rápido desde cualquier nodo.\n",
    "\n",
    "Ventajas:\n",
    "\n",
    "- Mayor velocidad de acceso: Reducción del tiempo necesario para acceder a datos frecuentemente utilizados.\n",
    "- Mejor rendimiento global: Optimización del uso de la memoria caché y reducción de la latencia.\n",
    "\n",
    "Desafíos:\n",
    "\n",
    "- Administración de caché: Determinar qué datos almacenar y cuándo reemplazar datos en la caché puede ser complicado.\n",
    "- Consistencia de caché: Mantener la coherencia de los datos almacenados en caché en un entorno distribuido puede ser difícil y costoso en términos de recursos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55973d9d-1ad8-4bd4-bdde-1cc192d7c9ce",
   "metadata": {},
   "source": [
    "#### Ejemplos\n",
    "\n",
    "- En un sistema de memoria distribuida, podríamos tener un clúster de computadoras donde cada nodo del clúster tiene su propia memoria local. Para resolver un problema conjunto, como un análisis de grandes datos, los nodos deben intercambiar información mediante mensajes. Si un nodo realiza un cálculo que otros nodos necesitan, debe enviar los resultados a los nodos pertinentes.\n",
    "\n",
    "- Un ejemplo práctico de memoria distribuida es un sistema de recomendación en un clúster de Hadoop. Cada nodo del clúster procesa una parte de los datos de usuario y elementos para generar recomendaciones. Los nodos deben intercambiar datos sobre usuarios y elementos similares para mejorar la precisión de las recomendaciones. Utilizando MPI, los desarrolladores pueden implementar un algoritmo que coordina la comunicación entre nodos para compartir información relevante."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f38628cd-355b-4bec-a415-a3c65ff9f8e7",
   "metadata": {},
   "source": [
    "#### Comparación de modelos\n",
    "\n",
    "Latencia y rendimiento:\n",
    "\n",
    "- Memoria compartida: Baja latencia de acceso a memoria compartida, pero alta latencia y complejidad en la coherencia de caché.\n",
    "- Memoria distribuida: Baja latencia en acceso a memoria local, pero alta latencia en comunicación inter-procesador.\n",
    "\n",
    "Escalabilidad:\n",
    "\n",
    "- Memoria compartida: Escalabilidad limitada debido a la contención de memoria y la complejidad de mantener la coherencia.\n",
    "- Memoria distribuida: Mejor escalabilidad, adecuada para grandes clústeres, pero requiere manejo explícito de comunicación y sincronización.\n",
    "\n",
    "Complejidad de programación:\n",
    "\n",
    "- Memoria compartida: Más sencilla debido a la uniformidad de acceso a memoria, pero requiere mecanismos de sincronización.\n",
    "- Memoria distribuida: Más compleja debido a la necesidad de gestionar el paso de mensajes y la sincronización.\n",
    "\n",
    "**Aplicaciones adecuadas**:\n",
    "\n",
    " - Memoria compartida: Aplicaciones que requieren acceso rápido a datos compartidos y donde la coherencia puede ser gestionada eficientemente.\n",
    " - Memoria distribuida: Aplicaciones que pueden ser descompuestas en tareas independientes que requieren poca comunicación, o donde la comunicación puede ser optimizada mediante el paso de mensajes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6fc8e65-2892-4882-8fa2-ba6ba77f1272",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ea5cba35-bc60-4107-a54e-9a985c7d1d87",
   "metadata": {},
   "source": [
    "### Ejercicios\n",
    "\n",
    "1 . Coherencia de caché: Explica cómo se mantiene la coherencia de caché en un sistema de memoria compartida utilizando el protocolo MESI. Incluya ejemplos de transiciones de estados de caché cuando dos procesadores acceden y modifican la misma línea de caché.\n",
    "\n",
    "Respuesta Esperada:\n",
    "\n",
    "- Debea describir los cuatro estados del protocolo MESI (Modificado, Exclusivo, Compartido, Inválido) y explicar las transiciones de estados cuando:\n",
    "  * Un procesador lee una línea de caché por primera vez.\n",
    "  * Un segundo procesador lee la misma línea de caché.\n",
    "  * El primer procesador modifica la línea de caché.\n",
    "  * El segundo procesador intenta leer la línea de caché modificada.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b7ad6a-6659-4368-8f6a-21b8e1ec0ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tu respuesta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e5a9f9c-8666-452d-b6d3-055002dabaa0",
   "metadata": {},
   "source": [
    "2 . Implementa un programa en C utilizando POSIX threads (pthread) que demuestre el uso de mutexes para proteger una variable compartida. El programa debe crear varios hilos que incrementen una variable global compartida de manera segura."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c58b2e-b1c8-4d55-8294-0e5086b5ef77",
   "metadata": {},
   "outputs": [],
   "source": [
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "#include <pthread.h>\n",
    "\n",
    "#define NUM_THREADS 5\n",
    "#define NUM_INCREMENTS 1000000\n",
    "\n",
    "pthread_mutex_t mutex;\n",
    "int shared_variable = 0;\n",
    "\n",
    "void* increment(void* arg) {\n",
    "    for (int i = 0; i < NUM_INCREMENTS; i++) {\n",
    "        pthread_mutex_lock(&mutex);\n",
    "        shared_variable++;\n",
    "        pthread_mutex_unlock(&mutex);\n",
    "    }\n",
    "    pthread_exit(NULL);\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    pthread_t threads[NUM_THREADS];\n",
    "    pthread_mutex_init(&mutex, NULL);\n",
    "\n",
    "    for (int i = 0; i < NUM_THREADS; i++) {\n",
    "        pthread_create(&threads[i], NULL, increment, NULL);\n",
    "    }\n",
    "\n",
    "    for (int i = 0; i < NUM_THREADS; i++) {\n",
    "        pthread_join(threads[i], NULL);\n",
    "    }\n",
    "\n",
    "    pthread_mutex_destroy(&mutex);\n",
    "    printf(\"Final value of shared_variable: %d\\n\", shared_variable);\n",
    "    return 0;\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b026b45-fa27-4223-8f5d-5732ea619713",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tu respuesta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de1dc59b-4fb7-4e6e-86bd-302a9d6c68c1",
   "metadata": {},
   "source": [
    "3 . Describe los diferentes modelos de consistencia de memoria (consistencia estricta, consistencia secuencial, consistencia causal) y cómo afectan el comportamiento observable de los programas en un sistema de memoria compartida.\n",
    "\n",
    "Respuesta Esperada:\n",
    "\n",
    "Debes explicar:\n",
    "\n",
    "* Consistencia estricta: Todas las operaciones de memoria son vistas por todos los procesadores en el orden exacto en que ocurren.\n",
    "* Consistencia secuencial: Las operaciones de memoria de todos los procesadores se intercalan en un orden secuencial que es consistente con el orden de programa de cada procesador.\n",
    "* Consistencia causal: Solo las operaciones de memoria que son causalmente relacionadas deben ser vistas en el mismo orden por todos los procesadores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "617d4c3c-82bd-4512-b6a8-dae215782a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tu respuesta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "697adeb7-4567-44b9-9192-7de0aa28b78b",
   "metadata": {},
   "source": [
    "4 . Explica cómo el paso de mensajes en un sistema de memoria distribuida permite la comunicación entre nodos. Describa las ventajas y desventajas del paso de mensajes comparado con la memoria compartida.\n",
    "\n",
    "Respuesta Esperada:\n",
    "\n",
    "Debes explicar:\n",
    "\n",
    "- Cómo los nodos envían y reciben mensajes para compartir datos.\n",
    "- Ventajas: No requiere coherencia de caché, escalabilidad mejorada, adecuado para sistemas distribuidos.\n",
    "- Desventajas: Latencia en la comunicación, mayor complejidad en la programación, sobrecarga de comunicación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17dac008-14b6-430b-abee-20e5af73d878",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tu respuesta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f0e93a-071c-4334-aa23-ee519c4cd96f",
   "metadata": {},
   "source": [
    "5 . Compare los modelos de consistencia eventual y consistencia fuerte en sistemas de memoria distribuida. Proporcione ejemplos de aplicaciones donde cada modelo sería más adecuado.\n",
    "\n",
    "Respuesta Esperada:\n",
    "\n",
    "Debes describir:\n",
    "\n",
    "- Consistencia fuerte: Las actualizaciones son visibles instantáneamente a todos los nodos, proporcionando una vista consistente de los datos en todo momento.\n",
    "- Consistencia eventual: Las actualizaciones se propagan gradualmente y todos los nodos eventualmente alcanzan una consistencia, pero puede haber inconsistencias temporales.\n",
    "- Ejemplos: Consistencia fuerte es crucial para aplicaciones financieras, mientras que la consistencia eventual es adecuada para redes sociales y sistemas de caching distribuido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae25e92-ce59-4eda-b4d1-2e17fec7252e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tu respuesta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42bdc366-9378-4579-aace-35312e868e98",
   "metadata": {},
   "source": [
    "6 . Implementa un programa en Python que utilice el módulo multiprocessing para demostrar la memoria compartida. Crea varios procesos que incrementen una variable compartida de manera segura utilizando un Value y un Lock."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "413c321c-dac3-4399-82b1-c24f7a99b1a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "\n",
    "def increment(shared_value, lock):\n",
    "    for _ in range(10000):\n",
    "        with lock:\n",
    "            shared_value.value += 1\n",
    "\n",
    "def main():\n",
    "    shared_value = multiprocessing.Value('i', 0)\n",
    "    lock = multiprocessing.Lock()\n",
    "    processes = []\n",
    "\n",
    "    for _ in range(4):\n",
    "        p = multiprocessing.Process(target=increment, args=(shared_value, lock))\n",
    "        processes.append(p)\n",
    "        p.start()\n",
    "\n",
    "    for p in processes:\n",
    "        p.join()\n",
    "\n",
    "    print(f\"Final value: {shared_value.value}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262b4a0d-96e1-457b-ba15-8e8c44e49ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tu respuesta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7154f925-f0d4-4dc0-88a2-8db9110f21a9",
   "metadata": {},
   "source": [
    "7 . Explica la diferencia entre coherencia de caché y consistencia de caché. Proporciona ejemplos de cómo estos conceptos afectan el rendimiento de un sistema multiprocesador.\n",
    "\n",
    "Respuesta esperada:\n",
    "\n",
    "Debes explicar:\n",
    "\n",
    "- Coherencia de Caché: Garantiza que todas las copias de un dato en diferentes cachés sean iguales.\n",
    "- Consistencia de Caché: Garantiza el orden en que las operaciones de memoria son vistas por diferentes procesadores.\n",
    "- Ejemplos: Condiciones de carrera debido a la falta de coherencia, problemas de sincronización debido a la falta de consistencia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "570537b1-6155-4e4a-88c1-7892fb03b60a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tu respuesta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d6ccf4e-be80-44dd-869c-998f9591f225",
   "metadata": {},
   "source": [
    "8 . Implementa un programa en Python que simule la coherencia de caché utilizando threading. Crea un sistema donde múltiples hilos modifiquen una variable compartida y utilice bloqueos para garantizar la coherencia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee039d4a-e1c0-48f6-aa84-1e1ac32af929",
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "\n",
    "shared_value = 0\n",
    "lock = threading.Lock()\n",
    "\n",
    "def modify_shared_value():\n",
    "    global shared_value\n",
    "    for _ in range(10000):\n",
    "        with lock:\n",
    "            temp = shared_value\n",
    "            temp += 1\n",
    "            shared_value = temp\n",
    "\n",
    "def main():\n",
    "    threads = []\n",
    "\n",
    "    for _ in range(4):\n",
    "        t = threading.Thread(target=modify_shared_value)\n",
    "        threads.append(t)\n",
    "        t.start()\n",
    "\n",
    "    for t in threads:\n",
    "        t.join()\n",
    "\n",
    "    print(f\"Final value: {shared_value}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0104f595-3f9b-4841-bcda-732c5801ac5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tu respuesta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b44d92-e1d4-44e2-a6b2-33d4801e979b",
   "metadata": {},
   "source": [
    "9 .Describe cómo funciona un sistema de snoop bus para mantener la coherencia de caché en un sistema multiprocesador. ¿Cuáles son los desafíos asociados con el snoop bus?\n",
    "\n",
    "Respuesta Esperada:\n",
    "\n",
    "Debes explicar:\n",
    "\n",
    "* Funcionamiento del snoop bus: Todas las cachés observan (snooping) el bus de datos para detectar operaciones relevantes y mantener la coherencia.\n",
    "* Desafíos: Escalabilidad limitada debido al tráfico en el bus, latencia, complejidad de implementación.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4993081c-412c-4ef9-888f-3128a3b8a17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tu respuesta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42200824-bdcb-4d86-9d80-734eefc1c3e3",
   "metadata": {},
   "source": [
    "10 . Implementa un programa en Python que simule un sistema de snoop bus utilizando hilos. Cada hilo representa un núcleo con su propia caché y observa una lista compartida de operaciones de memoria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d65ee29-f545-44c1-939f-7c4b2a0046d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "import time\n",
    "\n",
    "shared_memory = [0] * 10\n",
    "bus_operations = []\n",
    "bus_lock = threading.Lock()\n",
    "\n",
    "class Cache:\n",
    "    def __init__(self, id):\n",
    "        self.id = id\n",
    "        self.cache = [0] * 10\n",
    "\n",
    "    def read(self, index):\n",
    "        with bus_lock:\n",
    "            bus_operations.append((self.id, 'read', index))\n",
    "        return self.cache[index]\n",
    "\n",
    "    def write(self, index, value):\n",
    "        with bus_lock:\n",
    "            bus_operations.append((self.id, 'write', index, value))\n",
    "        self.cache[index] = value\n",
    "\n",
    "    def snoop(self):\n",
    "        while True:\n",
    "            with bus_lock:\n",
    "                if bus_operations:\n",
    "                    op = bus_operations.pop(0)\n",
    "                    if op[1] == 'write':\n",
    "                        self.cache[op[2]] = op[3]\n",
    "            time.sleep(0.01)\n",
    "\n",
    "def cpu_task(cache, index, value):\n",
    "    cache.write(index, value)\n",
    "    print(f\"CPU {cache.id} wrote {value} at index {index}\")\n",
    "    time.sleep(1)\n",
    "    read_value = cache.read(index)\n",
    "    print(f\"CPU {cache.id} read {read_value} from index {index}\")\n",
    "\n",
    "def main():\n",
    "    caches = [Cache(i) for i in range(4)]\n",
    "    threads = []\n",
    "\n",
    "    for cache in caches:\n",
    "        t = threading.Thread(target=cache.snoop)\n",
    "        t.daemon = True\n",
    "        t.start()\n",
    "\n",
    "    for i, cache in enumerate(caches):\n",
    "        t = threading.Thread(target=cpu_task, args=(cache, i % 10, i))\n",
    "        threads.append(t)\n",
    "        t.start()\n",
    "\n",
    "    for t in threads:\n",
    "        t.join()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54dff81c-dc37-4f73-b2df-b743b0b991a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tu respuesta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acca8e02-842a-4c5c-b631-2d05d1e0e3bc",
   "metadata": {},
   "source": [
    "11 . Describe el protocolo MESI para la coherencia de caché. Explica los cuatro estados posibles y cómo las transiciones de estado aseguran la coherencia de los datos.\n",
    "\n",
    "Respuesta Esperada:\n",
    "\n",
    "Debes explicar:\n",
    "\n",
    "* Estados del Protocolo MESI: Modificado (M), Exclusivo (E), Compartido (S), Inválido (I).\n",
    "* Transiciones: Cómo y cuándo ocurren las transiciones entre estos estados basadas en las operaciones de lectura y escritura de los procesadores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a023ce53-208a-4b1d-9c18-109954e62b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tu respuesta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "526174e7-b18e-44a3-86be-38cb42e48d19",
   "metadata": {},
   "source": [
    "12 . Implementa un programa en Python que simule el protocolo MESI. Crea una clase CacheLine con los cuatro estados y simule operaciones de lectura y escritura que provoquen transiciones de estado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef26d95b-8322-4fb0-9630-25276311938c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CacheLine:\n",
    "    def __init__(self):\n",
    "        self.state = 'I'  # Initial state is Invalid\n",
    "        self.value = None\n",
    "\n",
    "    def read(self):\n",
    "        if self.state == 'I':\n",
    "            self.state = 'S'\n",
    "            print(\"Transition to Shared\")\n",
    "        return self.value\n",
    "\n",
    "    def write(self, value):\n",
    "        if self.state in ('I', 'S'):\n",
    "            self.state = 'M'\n",
    "            print(\"Transition to Modified\")\n",
    "        self.value = value\n",
    "\n",
    "    def get_state(self):\n",
    "        return self.state\n",
    "\n",
    "def main():\n",
    "    cache_line = CacheLine()\n",
    "\n",
    "    # Simulate write operation\n",
    "    print(\"Writing value 42\")\n",
    "    cache_line.write(42)\n",
    "    print(f\"State after write: {cache_line.get_state()}\")\n",
    "\n",
    "    # Simulate read operation\n",
    "    print(\"Reading value\")\n",
    "    value = cache_line.read()\n",
    "    print(f\"State after read: {cache_line.get_state()}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7427b0bb-2962-46b0-8217-b56a97b4c3c4",
   "metadata": {},
   "source": [
    "13 . Implementa un programa en C usando OpenMP que utilice una barrera para sincronizar los hilos en diferentes fases de un cálculo. El programa debe dividir un cálculo en dos fases y asegurarse de que todos los hilos completen la primera fase antes de pasar a la segunda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31174b7-3dd9-4c06-87bd-2b7822e9734c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#include <stdio.h>\n",
    "#include <omp.h>\n",
    "\n",
    "#define NUM_THREADS 4\n",
    "#define ARRAY_SIZE 16\n",
    "\n",
    "int main() {\n",
    "    int array[ARRAY_SIZE];\n",
    "    for (int i = 0; i < ARRAY_SIZE; i++) {\n",
    "        array[i] = i;\n",
    "    }\n",
    "\n",
    "    omp_set_num_threads(NUM_THREADS);\n",
    "\n",
    "    #pragma omp parallel\n",
    "    {\n",
    "        int id = omp_get_thread_num();\n",
    "        int nthreads = omp_get_num_threads();\n",
    "\n",
    "        // Fase 1: Sumar 10 a cada elemento del array\n",
    "        for (int i = id; i < ARRAY_SIZE; i += nthreads) {\n",
    "            array[i] += 10;\n",
    "        }\n",
    "\n",
    "        // Sincronización de barrera\n",
    "        #pragma omp barrier\n",
    "\n",
    "        // Fase 2: Multiplicar cada elemento por 2\n",
    "        for (int i = id; i < ARRAY_SIZE; i += nthreads) {\n",
    "            array[i] *= 2;\n",
    "        }\n",
    "    }\n",
    "\n",
    "    printf(\"Array final:\\n\");\n",
    "    for (int i = 0; i < ARRAY_SIZE; i++) {\n",
    "        printf(\"%d \", array[i]);\n",
    "    }\n",
    "    printf(\"\\n\");\n",
    "\n",
    "    return 0;\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d73c96-fe9a-4e23-a87e-4c72f438d4de",
   "metadata": {},
   "source": [
    "## Tu respuesta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19dc1e81-c585-4531-92bd-f0e8f5f73fec",
   "metadata": {},
   "source": [
    "14 . Implementa un programa en C utilizando MPI (Message Passing Interface) para sumar los elementos de un array distribuido entre varios procesos. Cada proceso calcula la suma parcial de su porción y luego los resultados parciales se combinan en el proceso raíz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11caff30-b2e8-450a-b0e1-4dad5e67fbb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#include <mpi.h>\n",
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "\n",
    "#define ARRAY_SIZE 100\n",
    "#define ROOT 0\n",
    "\n",
    "int main(int argc, char** argv) {\n",
    "    MPI_Init(&argc, &argv);\n",
    "\n",
    "    int rank, size;\n",
    "    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n",
    "    MPI_Comm_size(MPI_COMM_WORLD, &size);\n",
    "\n",
    "    int local_size = ARRAY_SIZE / size;\n",
    "    int* array = NULL;\n",
    "    int* local_array = (int*)malloc(local_size * sizeof(int));\n",
    "\n",
    "    if (rank == ROOT) {\n",
    "        array = (int*)malloc(ARRAY_SIZE * sizeof(int));\n",
    "        for (int i = 0; i < ARRAY_SIZE; i++) {\n",
    "            array[i] = i + 1;\n",
    "        }\n",
    "    }\n",
    "\n",
    "    MPI_Scatter(array, local_size, MPI_INT, local_array, local_size, MPI_INT, ROOT, MPI_COMM_WORLD);\n",
    "\n",
    "    int local_sum = 0;\n",
    "    for (int i = 0; i < local_size; i++) {\n",
    "        local_sum += local_array[i];\n",
    "    }\n",
    "\n",
    "    int global_sum;\n",
    "    MPI_Reduce(&local_sum, &global_sum, 1, MPI_INT, MPI_SUM, ROOT, MPI_COMM_WORLD);\n",
    "\n",
    "    if (rank == ROOT) {\n",
    "        printf(\"Total sum: %d\\n\", global_sum);\n",
    "        free(array);\n",
    "    }\n",
    "\n",
    "    free(local_array);\n",
    "    MPI_Finalize();\n",
    "\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6be774a-d524-4d84-80ce-5d01d57e3989",
   "metadata": {},
   "source": [
    "15 . Implementa un programa en C utilizando POSIX threads (pthread) que implemente el algoritmo de productor-consumidor con un búfer compartido. Usa mutexes y condiciones para sincronizar los accesos al búfer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fffa3ea-2f9c-4a51-8cad-c61a922d8d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "#include <pthread.h>\n",
    "#include <unistd.h>\n",
    "\n",
    "#define BUFFER_SIZE 10\n",
    "\n",
    "int buffer[BUFFER_SIZE];\n",
    "int count = 0;\n",
    "pthread_mutex_t mutex;\n",
    "pthread_cond_t cond_producer, cond_consumer;\n",
    "\n",
    "void* producer(void* arg) {\n",
    "    for (int i = 0; i < 20; i++) {\n",
    "        pthread_mutex_lock(&mutex);\n",
    "\n",
    "        while (count == BUFFER_SIZE) {\n",
    "            pthread_cond_wait(&cond_producer, &mutex);\n",
    "        }\n",
    "\n",
    "        buffer[count++] = i;\n",
    "        printf(\"Produced: %d\\n\", i);\n",
    "\n",
    "        pthread_cond_signal(&cond_consumer);\n",
    "        pthread_mutex_unlock(&mutex);\n",
    "\n",
    "        sleep(rand() % 2);\n",
    "    }\n",
    "    pthread_exit(NULL);\n",
    "}\n",
    "\n",
    "void* consumer(void* arg) {\n",
    "    for (int i = 0; i < 20; i++) {\n",
    "        pthread_mutex_lock(&mutex);\n",
    "\n",
    "        while (count == 0) {\n",
    "            pthread_cond_wait(&cond_consumer, &mutex);\n",
    "        }\n",
    "\n",
    "        int item = buffer[--count];\n",
    "        printf(\"Consumed: %d\\n\", item);\n",
    "\n",
    "        pthread_cond_signal(&cond_producer);\n",
    "        pthread_mutex_unlock(&mutex);\n",
    "\n",
    "        sleep(rand() % 3);\n",
    "    }\n",
    "    pthread_exit(NULL);\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    pthread_t prod_thread, cons_thread;\n",
    "    pthread_mutex_init(&mutex, NULL);\n",
    "    pthread_cond_init(&cond_producer, NULL);\n",
    "    pthread_cond_init(&cond_consumer, NULL);\n",
    "\n",
    "    pthread_create(&prod_thread, NULL, producer, NULL);\n",
    "    pthread_create(&cons_thread, NULL, consumer, NULL);\n",
    "\n",
    "    pthread_join(prod_thread, NULL);\n",
    "    pthread_join(cons_thread, NULL);\n",
    "\n",
    "    pthread_mutex_destroy(&mutex);\n",
    "    pthread_cond_destroy(&cond_producer);\n",
    "    pthread_cond_destroy(&cond_consumer);\n",
    "\n",
    "    return 0;\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a695ea0-0c49-41f0-8d0e-a4216e7c778d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tu respuesta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6649d5c3-b4bb-4b5f-9ecb-596ffd3bff18",
   "metadata": {},
   "source": [
    "16. Discute cómo los algoritmos pueden ser optimizados para sistemas de memoria compartida. Incluye estrategias como la localización de datos, la reducción de contención y la minimización de la latencia de caché.\n",
    "\n",
    "Respuesta Esperada:\n",
    "\n",
    "Debea explicar:\n",
    "\n",
    "* Localización de datos: Mantener los datos que son accedidos frecuentemente juntos en memoria para aprovechar la caché.\n",
    "* Reducción de contención: Usar estructuras de datos concurrentes optimizadas y particionar tareas para minimizar el acceso simultáneo a la misma memoria.\n",
    "* Minimización de la latencia de caché: Acceder a los datos en patrones que maximicen la eficiencia de la caché, evitando el \"cache thrashing\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a41dada4-f5fb-4b6e-b8ec-7be11bcbc0c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tu respuesta"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
